{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Module2_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chilli3012/IIIT_Work/blob/main/Copy_of_Module2_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad**\n",
        "### MODULE 2: Feature Normalization, Nearest Neighbor Revisited\n",
        "### Project: Binary Classification of Adults \n",
        "#### Module Coordinator: Tanvi Kamble\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "anDuinxY0UVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This projects requires you to apply the machine learning cocepts that you learnt so far to fill in the #TODO parts so that we can classify which income group an adult lies in. \n",
        "\n",
        "An adult's income can be determined by a lot of factors like the individualâ€™s education level, age, gender, occupation, and etc. We use a dataset prsent on Kaggle provided by [UCI](http://www.cs.toronto.edu/~delve/data/adult/desc.html) to perform KNN and find the income group. "
      ],
      "metadata": {
        "id": "oZzgySXEOBDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's open the dataset stored as a CSV file using pandas dataframe, stored in google drive."
      ],
      "metadata": {
        "id": "2L56R2feQBMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0TI5BG3zsp-",
        "outputId": "157e5360-7099-4bc7-bb52-0e1c964fe4ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OkPeCRuiQn-k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adult = pd.read_csv('/content/drive/MyDrive/adult.csv')"
      ],
      "metadata": {
        "id": "q8VtsLJGztlG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "5874e224-8f47-469f-8b5e-e15f75d06acc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-39a892e24437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/adult.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/adult.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the dataset\n",
        "adult.head(10)"
      ],
      "metadata": {
        "id": "phVLiLi3QR8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicate entries\n",
        "adult=adult.drop_duplicates()"
      ],
      "metadata": {
        "id": "t1vFnnM3wyQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get to know the dataset\n",
        "adult.info()"
      ],
      "metadata": {
        "id": "E5bH6y-MRb5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adult.describe().T"
      ],
      "metadata": {
        "id": "OTKOPabtRjdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Index Column so that each entry is identified independently\n",
        "adult['Index'] = range(1, len(adult) + 1)"
      ],
      "metadata": {
        "id": "4mVHJvxvwhXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adult = adult.set_index('Index')"
      ],
      "metadata": {
        "id": "KyBkdgGw5Deg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset has '?' in place of all Null entries. Let's find the total null entries."
      ],
      "metadata": {
        "id": "q2ooDUwzpYoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adult.isin(['?']).sum()"
      ],
      "metadata": {
        "id": "fWwdoDJ-RrHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = adult.copy()"
      ],
      "metadata": {
        "id": "LECPMJdv6_eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['income'] = df['income'].replace('nan', np.nan)\n",
        "df = df[df['income'].isin([np.nan]) == False]"
      ],
      "metadata": {
        "id": "44lERS3q8VxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Three classes called Workclass, Occupation and Native-Country have null values so we first replace it with np.nan. \n",
        "df['workclass']=df['workclass'].replace('?',np.nan)\n",
        "df['occupation']=df['occupation'].replace('?',np.nan)\n",
        "df['native-country']=df['native-country'].replace('?',np.nan)"
      ],
      "metadata": {
        "id": "Z8eRbJirppV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These three features are categorical in nature so performing Imputation based KNN will be the best option to find out the missing features. "
      ],
      "metadata": {
        "id": "MXSCvft4u0yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb=LabelEncoder()\n",
        "df.education=lb.fit_transform(df.education)\n",
        "df['marital-status']=lb.fit_transform(df['marital-status'])\n",
        "df.relationship=lb.fit_transform(df.relationship)\n",
        "df.race=lb.fit_transform(df.race)\n",
        "df.gender=lb.fit_transform(df.gender)\n",
        "df.income=lb.fit_transform(df.income)"
      ],
      "metadata": {
        "id": "JUgu_QI3sLMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isin([np.nan]).sum()"
      ],
      "metadata": {
        "id": "XQlS0KedtW_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the NULL values of capital loss and hours per week feature perfrom imputation by mean. \n",
        "df['capital-loss'] = #TODO\n",
        "df['hours-per-week'] = #TODO"
      ],
      "metadata": {
        "id": "_bQ7qrqivmkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPUTATION USING K-NN\n",
        "# Workclass\n",
        "x_train_workclass = df.loc[df['workclass'].isin([np.nan]) == False].drop(['workclass', 'occupation', 'native-country'], axis = 1) \n",
        "y_train_workclass = df.loc[df['workclass'].isin([np.nan]) == False].workclass\n",
        "y_train_workclass = lb.fit_transform(y_train_workclass)\n",
        "for itr, ind in enumerate(x_train_workclass.index):\n",
        "  df['workclass'][ind] = y_train_workclass[itr]\n",
        "x_test_workclass = df.loc[df['workclass'].isin([np.nan])].drop(['workclass', 'occupation', 'native-country'], axis = 1) \n",
        "# Occupation\n",
        "x_train_occupation = df.loc[df['occupation'].isin([np.nan]) == False].drop(['workclass', 'occupation', 'native-country'], axis = 1) \n",
        "y_train_occupation = df.loc[df['occupation'].isin([np.nan]) == False].occupation\n",
        "y_train_occupation = lb.fit_transform(y_train_occupation)\n",
        "for itr, ind in enumerate(x_train_occupation.index):\n",
        "  df['occupation'][ind] = y_train_occupation[itr]\n",
        "x_test_occupation = df.loc[df['occupation'].isin([np.nan])].drop(['workclass', 'occupation', 'native-country'], axis = 1) \n",
        "# Native Country\n",
        "x_train_country = df.loc[df['native-country'].isin([np.nan]) == False].drop(['workclass', 'occupation', 'native-country'], axis = 1) \n",
        "y_train_country = df.loc[df['native-country'].isin([np.nan]) == False]['native-country']\n",
        "y_train_country = lb.fit_transform(y_train_country)\n",
        "for itr, ind in enumerate(x_train_country.index):\n",
        "  df['native-country'][ind] = y_train_country[itr]\n",
        "x_test_country = df.loc[df['native-country'].isin([np.nan])].drop(['workclass', 'occupation', 'native-country'], axis = 1) "
      ],
      "metadata": {
        "id": "yWZSNxWNwUWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "dUZZRbEy0zNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating predictions for all the features\n",
        "#  Use the KNeighborsClassifier with neighbours = 7 and all the other entries as default to find the missing values.\n",
        "'''\n",
        " TODO:\n",
        " for each of the features:\n",
        "  define a knn classifier with k = 7 \n",
        "  Fit the training data into the model \n",
        "  find the predictions\n",
        "\n",
        "'''\n",
        "workplace_pred = None\n",
        "occupation_pred = None\n",
        "country_pred = None\n",
        "\n"
      ],
      "metadata": {
        "id": "XFbZQYhp2Da0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the predicted values in the original dataframe\n",
        "for itr, ind in enumerate(x_test_workclass.index):\n",
        "  df['workclass'][ind] = workplace_pred[itr]\n",
        "\n",
        "for itr, ind in enumerate(x_test_occupation.index):\n",
        "  df['occupation'][ind] = occupation_pred[itr]\n",
        "\n",
        "for itr, ind in enumerate(x_test_country.index):\n",
        "  df['native-country'][ind] = country_pred[itr]"
      ],
      "metadata": {
        "id": "w8xZC5535Vl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['workclass'] = df['workclass'].astype(str).astype(int)\n",
        "df['occupation'] = df['occupation'].astype(str).astype(int)\n",
        "df['native-country'] = df['native-country'].astype(str).astype(int)"
      ],
      "metadata": {
        "id": "fbgHbL00x6pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = df.hist(figsize = (15,15))"
      ],
      "metadata": {
        "id": "dDS8Fq4CrhzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After plotting the figures we can see that there is some scope for clipping fnlwgt, capital loss and capital gain with vmax. "
      ],
      "metadata": {
        "id": "2SJILFi_8Mit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "TODO\n",
        "Choose an appropriate maximum value to clip Capital Loss and Capital Gain values to and clip them accordingly\n",
        "'''\n",
        "df_standard = df.copy()\n",
        "vmax_cap_gain = 0\n",
        "vmax_cap_loss = 0\n",
        "vmax_fnlwgt = 0\n",
        "df_standard['capital-loss'] = df_standard['capital-loss']\n",
        "df_standard['capital-gain'] = df_standard['capital-gain']\n",
        "df_standard['fnlwgt'] = df_standard['fnlwgt']"
      ],
      "metadata": {
        "id": "fZ1ijzhirya4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_standard = df_standard.dropna(how = 'all')"
      ],
      "metadata": {
        "id": "Vr3JeGCt_Dji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the same graph for standardized data\n",
        "p = df_standard.hist(figsize = (15,15))"
      ],
      "metadata": {
        "id": "AlIgPaU0SXC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the various features now and see if we can find any useless features not required for KNN\n",
        "from pandas.plotting import scatter_matrix\n",
        "p = scatter_matrix(df,figsize=(25, 25))\n"
      ],
      "metadata": {
        "id": "BiYtW06S9FPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's analyse the same using correlation map. \n",
        "df.corr()"
      ],
      "metadata": {
        "id": "koPGhrtP1Bh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "For the income column it is clear that no column directly affects the Income. We can safely assume that there no feature will completely overpower and determine the outcome. Hence, no need for regularization.  "
      ],
      "metadata": {
        "id": "oSfrgf0Q0mW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the data is biased\n",
        "print(df['income'].value_counts())\n",
        "plt.bar([0,1],df['income'].value_counts())"
      ],
      "metadata": {
        "id": "uOESUhFc2f-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "There is a clear bias in the data and our model might end up giving income as 0 for most cases"
      ],
      "metadata": {
        "id": "x_qS7liB3TQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we want to Z-Scale the data \n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "0fZj71Jn9P6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Write the code for z-transforming the data using StandardScalar class of sklearn which we have imported in the last cell. \n",
        "scaled_data = None"
      ],
      "metadata": {
        "id": "d-TANglj9-Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled = pd.DataFrame(scaled_data,columns=df_standard.columns[:-1])\n",
        "df_scaled.head()"
      ],
      "metadata": {
        "id": "nvFbtC_T6Cjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix"
      ],
      "metadata": {
        "id": "t8sen5Yz35-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a K-NN and compare the performances of scaled vs unscaled data. \n",
        "# We first create a function for performing KNN\n",
        "##########################\n",
        "##########################\n",
        "\n",
        "## TODO : Complete the lines of code wherever marked as [REQUIRED] in this cell.\n",
        "\n",
        "##########################\n",
        "##########################\n",
        "\n",
        "\n",
        "def plot_KNN_error_rate(xdata,ydata):\n",
        "  error_rate = []\n",
        "  test_scores = []\n",
        "  train_scores = []\n",
        "\n",
        "  ## [REQUIRED] Split the data into train and test sets in a 70:30 ratio (70% train, 30% test)\n",
        "  X_train, X_test, y_train, y_test = ## Write your code here (expected lines ~ 1)\n",
        "  \n",
        "  for i in range(1,15):\n",
        "      ## [REQUIRED] Complete the code in the next three lines\n",
        "      knn = ## Write your code here. Initialize the KNN classifier with 'i' neighbours (expected lines ~ 1)\n",
        "      ## Write your code here. Fit the KNN model on the training set (expected lines ~ 1)\n",
        "      pred_i = ## Write your code here. Make predictions on the test set using KNN (expected lines ~ 1)\n",
        "      \n",
        "      error_rate.append(np.mean(pred_i != y_test))\n",
        "      train_scores.append(knn.score(X_train,y_train))\n",
        "      test_scores.append(knn.score(X_test,y_test))\n",
        "\n",
        "  plt.figure(figsize=(12,8))\n",
        "  plt.plot(range(1,15),error_rate,color='blue', linestyle='dashed', marker='o',\n",
        "          markerfacecolor='red', markersize=10)\n",
        "  plt.title('Error Rate vs. K Value')\n",
        "  plt.xlabel('K')\n",
        "  plt.ylabel('Error Rate')\n",
        "  print()\n",
        "  ## score that comes from testing on the same datapoints that were used for training\n",
        "  max_train_score = max(train_scores)\n",
        "  train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\n",
        "  print('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))\n",
        "  print()\n",
        "  ## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\n",
        "  max_test_score = max(test_scores)\n",
        "  test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\n",
        "  print('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))\n",
        "\n",
        "  return test_scores\n"
      ],
      "metadata": {
        "id": "6LWLyX86-egU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unchanged dataset\n",
        "orig_X = df.drop('income', axis = 1)\n",
        "orig_y = df.income\n",
        "unchanged_test_scores = plot_KNN_error_rate(orig_X, orig_y)"
      ],
      "metadata": {
        "id": "yhL-4qTI0V7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardized Dataset\n",
        "scaled_X = df_scaled\n",
        "scaled_y = df_standard.income\n",
        "scaled_test_scores = plot_KNN_error_rate(scaled_X, scaled_y)"
      ],
      "metadata": {
        "id": "0qw81sO730ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing the two accuracies\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.title('Accuracy vs. K Value')\n",
        "sns.lineplot(range(1,15),unchanged_test_scores,marker='o',label='Unscaled data test score')\n",
        "sns.lineplot(range(1,15),scaled_test_scores,marker='o',label='Scaled data test Score')"
      ],
      "metadata": {
        "id": "JEMi8P284NvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO \n",
        "# Use Weighted KNN and compare the results of both the datasets"
      ],
      "metadata": {
        "id": "jBsom3RF4WD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Refer to MinMax Scaler provided in scikit-learn.  \n",
        "## Use MinMax scaling on the dataset, and see the performance of KNN on this minmax-scaled dataset."
      ],
      "metadata": {
        "id": "jT8F6qnM5rXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK-8: Use K-Fold cross validation on all the above classification experiments and present an analysis of the results you obtain."
      ],
      "metadata": {
        "id": "DoXqx9yS4bGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "We carried out data analysis which helped us realise the missing values and helped us check if there is any visible bias in the data. \n",
        "\n",
        "As for the classification tasks, the standardized data yields much better results than the unscaled data over most of the K-values considered, thus indicating the importance of standardizing data in Machine Learning problems."
      ],
      "metadata": {
        "id": "qZkVVbH7Gefa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "https://www.kaggle.com/wenruliu/adult-income-dataset"
      ],
      "metadata": {
        "id": "xvzA9gwYHDpF"
      }
    }
  ]
}